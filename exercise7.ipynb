{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"wZNtpeXtN6xU","executionInfo":{"status":"ok","timestamp":1674925632049,"user_tz":-60,"elapsed":4873,"user":{"displayName":"Diana D","userId":"17433485028620846395"}}},"outputs":[],"source":["%config InlineBackend.figure_formats = ['svg']\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as grid\n","\n","import itertools\n","import time"]},{"cell_type":"markdown","metadata":{"id":"MD-VoPb7N6xn"},"source":["# Structural Invariant and Equivariant Neural Networks\n","\n","## Motivation\n","Strucutral approaches try to achieve invariance/equivariance \n","**Definition**: Given a set $X:=\\{x_1,x_2,\\dots,x_n\\}\\in\\Bbb{R}^{n\\times d_q}$ with $x_i\\in\\Bbb{R}^{d_q}$, a set-based function is defined as:\n","                        $$f := \\Bbb{R}^{n\\times d_q}\\rightarrow\\Bbb{R}$$\n","                        \n"," <br> <br>                      \n","                        \n","**Properties**: A set-based function $f$ should be: \n","    - permutation-invariant: \n","$$f(X)= f(\\pi(X))$$\n","    - permutation-equivariant:\n","$$\\pi(f(X))= f(\\pi(X))$$, s.t. $$\\pi(X):= \\{x_{\\pi(1)},x_{\\pi(2)},\\dots,x_{\\pi(n)}\\}$$\n","    - process input sets of any size:\n","$$f := \\Bbb{R}^{n\\times d_q}\\rightarrow\\Bbb{R}, \\forall n\\in\\Bbb{N}$$\n","    \n","Work in this notebook:\n","\n","[1]  [Deep Sets](https://papers.nips.cc/paper/6931-deep-sets.pdf)\n","\n","[2]  [Universal approximiations of invariant/equivariant functions by deep neural networks](https://arxiv.org/pdf/1903.01939.pdf)"]},{"cell_type":"markdown","metadata":{"id":"ToKTXu73N6xw"},"source":["# 1. Structural Invariance\n","\n","**Kolmogorovâ€“Arnold representation:**\n","Let  $ f : [0, 1]^M\\rightarrow \\mathbb{R}$ be an arbitrary multivariate\n","continuous function iff it has the representation \n","\n","$$f(x_1,...,x_M)=\\rho\\left(\\sum_{m=1}^{M} \\lambda_m(\\phi(x_m)\\right)$$\n","\n","\n","with continuous outer and inner functions $\\rho: \\mathbb{R}^{2M+1} \\rightarrow \\mathbb{R} $ and $\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}^{2M+1}$. The inner function $\\phi$ is independent of the function $f$.\n","\n","**Deep Sets**: Represent $\\rho$ and $\\phi$ as feedforward neural networks. \n","\n","![DeepSets.png](attachment:d7d50434-2af8-466b-9ded-82df8b0e6a8c.png)"]},{"cell_type":"markdown","metadata":{"id":"hnocd_nFN6xz"},"source":["Deep Sets architecture as shown in [2]"]},{"cell_type":"markdown","metadata":{"id":"uDmLu0TcN6x2"},"source":["# Application: Image Digit Sums\n","**Problem definition** Given a set of images of handwritten digits we want to compute the sum of all digits. This problem is permuation invariant since permuting the images will not affect the digit sum.\n","For simplicity we will first embed the MNIST dataset to a latent space such that the models can operate on vector data.\n","\n","### Generate Data:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k8vY5R35N6x4","executionInfo":{"status":"ok","timestamp":1674925678520,"user_tz":-60,"elapsed":46483,"user":{"displayName":"Diana D","userId":"17433485028620846395"}},"outputId":"4bf50395-cab6-4511-989b-d7ed9fffb0cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1875/1875 [==============================] - 18s 8ms/step - loss: 0.3884 - accuracy: 0.8880 - val_loss: 0.2313 - val_accuracy: 0.9307\n","Epoch 2/5\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.3119 - accuracy: 0.9171 - val_loss: 0.2314 - val_accuracy: 0.9384\n","Epoch 3/5\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.2809 - accuracy: 0.9279 - val_loss: 0.1832 - val_accuracy: 0.9544\n","Epoch 4/5\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.2703 - accuracy: 0.9322 - val_loss: 0.2577 - val_accuracy: 0.9482\n","Epoch 5/5\n","1875/1875 [==============================] - 6s 3ms/step - loss: 0.2676 - accuracy: 0.9334 - val_loss: 0.1787 - val_accuracy: 0.9628\n"]}],"source":["# Number of digits in the set\n","digit_length = 5\n","\n","# Load the MNIST data from TF\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","# Use a simple model to train on MNIST to generate embeddings\n","embed_model = tf.keras.models.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(28, 28)),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(64,name=\"Embedding\"),\n","    tf.keras.layers.Dense(10)\n","])\n","embed_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(lr=1e-2),  # Optimizer\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=['accuracy']\n",")\n","\n","# Train on MNIST\n","embed_model.fit(x_train, y_train, epochs=5,validation_data=(x_test,y_test))\n","\n","# Embed the data\n","embed_model = Model(embed_model.input, outputs=embed_model.get_layer(\"Embedding\").output)\n","embed_train_x = embed_model(x_train).numpy()\n","embed_test_x = embed_model(x_test).numpy()\n","\n","\n","\"\"\"\n","    Takes the embedded MNIST data and returns a dataset for digit sum prediction. Each instance is a set of embedded instances with the label being the sum.\n","\"\"\"\n","def getDigitSum(digit_length,train_x,train_y,test_x,test_y):\n","    p = np.random.permutation(len(train_x))\n","    train_x = train_x[p]\n","    train_y = train_y[p]\n","    train_x = np.reshape(train_x,[-1,digit_length,64])\n","    train_y = np.reshape(train_y,[-1,digit_length])\n","    train_y = np.sum(train_y,1)\n","\n","    p = np.random.permutation(len(test_x))\n","    test_x = test_x[p]\n","    test_y = test_y[p]\n","    test_x = np.reshape(test_x,[-1,digit_length,64])\n","    test_y = np.reshape(test_y,[-1,digit_length])\n","    test_y = np.sum(test_y,1)\n","    return train_x,train_y,test_x,test_y"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hibylRX5N6x7","executionInfo":{"status":"ok","timestamp":1674925687608,"user_tz":-60,"elapsed":279,"user":{"displayName":"Diana D","userId":"17433485028620846395"}}},"outputs":[],"source":["# Generate data\n","train_x,train_y,test_x,test_y = getDigitSum(digit_length, embed_train_x, y_train, embed_test_x, y_test)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Onjlo--N6x-","executionInfo":{"status":"ok","timestamp":1674925689565,"user_tz":-60,"elapsed":221,"user":{"displayName":"Diana D","userId":"17433485028620846395"}},"outputId":"fe733d04-469b-4c55-a9ef-9adefcd56b55"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(12000, 5, 64)"]},"metadata":{},"execution_count":4}],"source":["train_x.shape"]},{"cell_type":"markdown","metadata":{"id":"OcdRS30nN6x_"},"source":["#### a)  Implement a simple Deep set model, and simple LSTM and Feedforward models as baseline (note that the Feedforward model just needs to flatten the training data from Batchsize X Set size X Feature to Batchsize X Feature before using dense layers"]},{"cell_type":"code","source":["# TODO Implement models - DeepSet\n","class DeepSet(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(DeepSet, self).__init__()\n","        # self.linear_1 = tf.keras.layers.Dense(32, activation='relu')\n","        #TODO: impl. deepset layers\n","\n","    def call(self, inputs):\n","        # TODO: call dataset layers\n","        # x = self.linear_1(inputs)\n","        # x = tf.nn.relu(x)\n","        return self.linear_3(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"FFJrW02fSdFr","executionInfo":{"status":"error","timestamp":1674572456008,"user_tz":-120,"elapsed":13,"user":{"displayName":"Diana D","userId":"17433485028620846395"}},"outputId":"1f3e6176-5774-4676-9038-72299d56a5da"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-a12847aea03b>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    # return self.linear_3(x)\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"code","source":["# TODO Implement models - LSTM\n","class LSTM(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(LSTM, self).__init__()\n","        self.linear_1 = tf.keras.layers.DeepSet(32, activation='relu')\n","        self.linear_2 = tf.keras.layers.DeepSet(32)\n","        self.linear_3 = tf.keras.layers.DeepSet(32)\n","\n","    def call(self, inputs):\n","        x = self.linear_1(inputs)\n","        x = tf.nn.relu(x)\n","        x = self.linear_2(x)\n","        x = tf.nn.relu(x)\n","        return self.linear_3(x)"],"metadata":{"id":"XrnPEg01Plfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO Implement models - Dense\n","# note that the Feedforward model just needs to flatten the training data from Batchsize X Set size X Feature to Batchsize X Feature before using dense layers\n","class Dense(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(Dense, self).__init__()\n","        self.linear_1 = tf.keras.layers.Dense(32, activation='relu')\n","        self.linear_2 = tf.keras.layers.Dense(32)\n","        self.linear_3 = tf.keras.layers.Dense(32)\n","\n","    def call(self, inputs):\n","        x = self.linear_1(inputs)\n","        x = tf.nn.relu(x)\n","        x = self.linear_2(x)\n","        x = tf.nn.relu(x)\n","        return self.linear_3(x)"],"metadata":{"id":"h6p89-6GPlll"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"Z4Mx3-e5N6yC","executionInfo":{"status":"error","timestamp":1674571508257,"user_tz":-120,"elapsed":644,"user":{"displayName":"Diana D","userId":"17433485028620846395"}},"outputId":"2eb76573-8ac3-48ff-e956-361a554bd659"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-db3de316af13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize all three models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_ds\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mDeepSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_lstm\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DeepSet' is not defined"]}],"source":["# Initialize all three models\n","model_ds    = DeepSet()\n","model_lstm  = LSTM()\n","model_dense = Dense()\n","\n","model_ds.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n","    loss=tf.keras.losses.MeanAbsoluteError()\n",")\n","\n","model_lstm.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n","    loss=tf.keras.losses.MeanAbsoluteError()\n",")\n","\n","model_dense.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n","    loss=tf.keras.losses.MeanAbsoluteError()\n",")"]},{"cell_type":"markdown","metadata":{"id":"mjt_JXX0N6yD"},"source":["### Train models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IV1XT_a8N6yE","outputId":"fb35004a-a159-486b-ec53-2c1fa36d6aa5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished training deep set model in 90.26102614402771s\n","Finished training LSTM model in 134.73382329940796s\n","Finished training Dense model in 55.796677112579346s\n","\n","Deep Set model - Trainable parameters:  52661\n","LSTM model     - Trainable parameters:  81361\n","Dense model    - Trainable parameters: 137089\n"]}],"source":["start = time.time()\n","hist_ds    = model_ds.fit(train_x, train_y, epochs=100, batch_size=128, shuffle=True,validation_data=(test_x,test_y),verbose=0)\n","print(f\"Finished training deep set model in {time.time()-start}s\")\n","start = time.time()\n","hist_lstm  = model_lstm.fit(train_x, train_y, epochs=100, batch_size=128, shuffle=True, validation_data=(test_x,test_y),verbose=0)\n","print(f\"Finished training LSTM model in {time.time()-start}s\")\n","start = time.time()\n","hist_dense = model_dense.fit(train_x, train_y, epochs=100, batch_size=128, shuffle=True, validation_data=(test_x,test_y),verbose=0)\n","print(f\"Finished training Dense model in {time.time()-start}s\")\n","print()\n","print(f\"Deep Set model - Trainable parameters:  {np.sum([np.prod(i.shape) for i in model_ds.trainable_weights])}\")\n","print(f\"LSTM model     - Trainable parameters:  {np.sum([np.prod(i.shape) for i in model_lstm.trainable_weights])}\")\n","print(f\"Dense model    - Trainable parameters: {np.sum([np.prod(i.shape) for i in model_dense.trainable_weights])}\")"]},{"cell_type":"markdown","metadata":{"id":"ZyNylQ82N6yF"},"source":["#### b) Visualize the test loss of the three models and print the score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3QxIGmuN6yG"},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"Zk5t9yOVN6yH"},"source":["### Does the model generalize across different set sizes?\n","\n","#### c) How does the model performance change with varying number of images in the set? Please evaluate the model trained on sets of length 5 by testing on sets of length 3 to 10. Plot the results in a graph"]},{"cell_type":"markdown","metadata":{"id":"gbsb5jRfN6yI"},"source":["# 2. Structural Equivariant\n","\n","The authors of [2] propose a permutation equivariant network by extending on the deep sets approach.\n","Compute deep sets for $n-1$ features and combine that output with the left out feature. Repeat over all features. Note that the embedding through $\\phi$ can be computed once thus making the model computationally light.\n","\n","![Equiv.png](attachment:bf6b2524-651d-4528-b1e1-6b06a4e9466e.png)"]},{"cell_type":"markdown","metadata":{"id":"gdVmlxurN6yI"},"source":["Equivariant architecture based on deep sets as proposed by [2]"]},{"cell_type":"markdown","metadata":{"id":"wxyjjKAUN6yJ"},"source":["# Application: Knapsack Problen\n","**Problem definition** Given a set of item with a corresponding weight and value we want to select a subset of these that maximizes the value while the combiend weight is lower than the maximum capacity. We can formulate this problem as a supervised machine learning problem by using a model that takes as input a set of weights and values for M items and outputs a binary decision for each item whether it is selected. This problem is permuation equivariant since permuting the order of items should only change the order of the predicted selection.\n","\n","### Generate Data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lgEHaHeN6yJ"},"outputs":[],"source":["num_items  = 8\n","train_size = 10000\n","val_size   = 10000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVdRVbQtN6yK"},"outputs":[],"source":["# Generate a knapsack problem with \"item_count\" items which have a value between 1-99 and a weight between 0-0.6. \n","# The optimal solution is binary vector of selected items for which the combined weight is smaller 1.0.\n","def knapsack_data(item_count=5):\n","    weights  = np.random.rand(item_count)*0.6\n","    values   = np.random.randint(1, 99, item_count)\n","    optimal  = brute_optimal(weights,values)\n","    return weights, values, optimal\n","\n","# We can find an optimal solution for small problems by iterating all possible selections and selecting the one with highest value after pruning for weight.\n","def brute_optimal(weights,values):\n","    item_count = len(weights)\n","    lst = list(itertools.product([0, 1], repeat=item_count))\n","    \n","    total_value  = np.matmul(np.array(lst),values)\n","    total_weight = np.matmul(np.array(lst),weights)\n","    total_value[total_weight>1.0]=0\n","    \n","    return lst[np.argmax(total_value)]\n","\n","# Generate a dataset with \"train_size\" and \"val_size\" knapsack problems with \"num_items\" items per problem\n","def get_data(train_size,val_size,num_items):\n","    start = time.time()\n","    X_weights = []\n","    X_values  = []\n","    Y = []\n","\n","    for i in range(train_size):\n","        w,v,o = knapsack_data(num_items)\n","        X_weights.append(w)\n","        X_values.append(v)\n","        Y.append(o)\n","    X_weights = np.array(X_weights).astype(np.float32)\n","    X_values  = np.array(X_values).astype(np.float32)\n","    Y         = np.array(Y).astype(np.float32)\n","\n","    X_weights_val = []\n","    X_values_val  = []\n","    Y_val = []\n","\n","    for i in range(val_size):\n","        w,v,o = knapsack_data(num_items)\n","        X_weights_val.append(w)\n","        X_values_val.append(v)\n","        Y_val.append(o)\n","    X_weights_val = np.array(X_weights_val).astype(np.float32)\n","    X_values_val  = np.array(X_values_val).astype(np.float32)\n","    Y_val         = np.array(Y_val).astype(np.float32)\n","    print(f\"{np.round(time.time()-start,2)}s\")\n","    \n","    return X_weights, X_values, Y, X_weights_val, X_values_val, Y_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69vkWBWON6yL","outputId":"5122d6a0-d740-4973-875d-4a12a3ee6bce"},"outputs":[{"data":{"text/plain":["(array([0.41339083, 0.49876553, 0.55634462, 0.04829673, 0.22248408,\n","        0.30891427, 0.01676307, 0.56470752]),\n"," array([52, 65, 72, 81, 55, 33, 54, 28]),\n"," (0, 0, 1, 1, 1, 0, 1, 0))"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Example Knapsack\n","knapsack_data(num_items)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fH2K9k0IN6yN","outputId":"346fffea-ec37-4f49-8e4f-a127c4f81053"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.99s\n"]}],"source":["# Generate dataset\n","X_weights, X_values, Y, X_weights_val, X_values_val, Y_val = get_data(train_size,val_size,num_items)"]},{"cell_type":"markdown","metadata":{"id":"g3H5jAoxN6yO"},"source":["### Equivariance\n","\n","a) Implement the equivariant network proposed in [2] and then evaluate it on the Knapsack data provided. Compare against the dense model below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uz2nq0U6N6yP"},"outputs":[],"source":["class Dense(Model):\n","    \n","    def __init__(self,item_count):\n","        \n","        super(Dense, self).__init__()\n","        \n","        self.d1   = tf.keras.layers.Dense(64,activation=\"relu\")\n","        self.d2   = tf.keras.layers.Dense(64,activation=\"relu\")\n","        self.out   = tf.keras.layers.Dense(item_count,activation=\"sigmoid\")\n","\n","    @tf.function\n","    def call(self, inputs):\n","        \n","        weights,values = inputs\n","        con = tf.keras.layers.concatenate([weights,values],axis=-1)\n","        \n","        d1  = self.d1(con)\n","        d2  = self.d2(d1)\n","        out = self.out(d2)\n","        \n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epQlzNHIN6yP"},"outputs":[],"source":["# Initialize both models\n","model_dense = Dense(num_items)\n","\n","model_dense.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Optimizer\n","    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",")\n","\n","\n","model_equiv = EquiNet() #Todo\n","\n","model_equiv.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Optimizer\n","    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",")"]},{"cell_type":"markdown","metadata":{"id":"O92IX-BvN6yQ"},"source":["### Train models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dB051H0ON6yQ","outputId":"dfa36b5f-e96f-4c6c-d61f-1b0ab1e8a0da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished training Dense model in 26.301701545715332s\n","Finished training Equivariant model in 56.54321050643921s\n","\n","Equivariant model - Trainable parameters: 4385\n","Dense model       - Trainable parameters: 5768\n"]}],"source":["start = time.time()\n","hist_dense = model_dense.fit([X_weights,X_values], Y, epochs=100, batch_size=128, shuffle=True,validation_data=([X_weights_val,X_values_val],Y_val),verbose=0)\n","print(f\"Finished training Dense model in {time.time()-start}s\")\n","start = time.time()\n","hist_equiv = model_equiv.fit([X_weights,X_values], Y, epochs=100, batch_size=128, shuffle=True,validation_data=([X_weights_val,X_values_val],Y_val),verbose=0)\n","print(f\"Finished training Equivariant model in {time.time()-start}s\")\n","print()\n","print(f\"Equivariant model - Trainable parameters: {np.sum([np.prod(i.shape) for i in model_equiv.trainable_weights])}\")\n","print(f\"Dense model       - Trainable parameters: {np.sum([np.prod(i.shape) for i in model_dense.trainable_weights])}\")"]},{"cell_type":"markdown","metadata":{"id":"nMyBZUMUN6yR"},"source":["### Visualize results\n","\n","#### b) Print val results and plot val loss for both models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4HFQ_owN6yS"},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"7uQT35e-N6yS"},"source":["### Does the model generalize across different set sizes?\n","\n","c) How does the model performance change with varying number of items? Evaluate the performance of the model trained on problems with 8 items on sets of length 4 to 12 and visualize the results"]}],"metadata":{"kernelspec":{"display_name":"TF210","language":"python","name":"tf210"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}